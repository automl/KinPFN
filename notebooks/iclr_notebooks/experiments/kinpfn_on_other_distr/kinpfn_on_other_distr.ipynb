{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from kinpfn.priors import Batch\n",
    "from kinpfn.model import KINPFN\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.integrate import cumulative_trapezoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def multi_modal_distribution(\n",
    "    x, num_peaks, log_start, log_end, rng=np.random.default_rng(seed=np.random.seed())\n",
    "):\n",
    "    # Generate parameters for uniform distributions\n",
    "    centers = rng.uniform(log_start + 1, log_end - 1, num_peaks).astype(np.float32)\n",
    "    widths = rng.uniform(0.1, (log_end - log_start) / 5, num_peaks).astype(np.float32)\n",
    "    \n",
    "    distribution = np.zeros_like(x, dtype=np.float32)\n",
    "\n",
    "    for center, width in zip(centers, widths):\n",
    "        # Define the bounds of the uniform distribution\n",
    "        lower_bound = center - width / 2\n",
    "        upper_bound = center + width / 2\n",
    "\n",
    "        # Add the uniform distribution within its range\n",
    "        mask = (np.log(x) >= lower_bound) & (np.log(x) <= upper_bound)\n",
    "        distribution[mask] += 1.0 / width  # Constant value for uniform distribution\n",
    "\n",
    "    return distribution\n",
    "\n",
    "def sample_from_multi_modal_distribution(batch_size, seq_len, num_features):\n",
    "\n",
    "    xs = torch.zeros(batch_size, seq_len, num_features, dtype=torch.float32)\n",
    "    ys = torch.empty(batch_size, seq_len, dtype=torch.float32)\n",
    "\n",
    "    rng = np.random.default_rng(seed=np.random.seed())\n",
    "    log_start = -6\n",
    "    log_end = 15\n",
    "    x = np.logspace(log_start, log_end, seq_len, dtype=np.float32)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        num_peaks = rng.integers(2, 6)\n",
    "        own_pdf = multi_modal_distribution(x, num_peaks, log_start, log_end, rng=rng)\n",
    "        own_pdf /= np.trapz(own_pdf, x)\n",
    "        own_cdf = np.cumsum(own_pdf).astype(np.float32)\n",
    "        own_cdf /= own_cdf[-1]\n",
    "        inverse_cdf = interp1d(\n",
    "            own_cdf, x, bounds_error=False, fill_value=(x[0], x[-1]), kind=\"linear\"\n",
    "        )\n",
    "        uniform_samples = rng.uniform(0, 1, seq_len).astype(np.float32)\n",
    "        samples = inverse_cdf(uniform_samples).astype(np.float32)\n",
    "        ys[i] = torch.tensor(samples, dtype=torch.float32)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "def get_batch_new_distribution_prior(\n",
    "    batch_size, seq_len, num_features=1, hyperparameters=None, **kwargs\n",
    "):\n",
    "    xs, ys = sample_from_multi_modal_distribution(\n",
    "        batch_size=batch_size, seq_len=seq_len, num_features=num_features\n",
    "    )\n",
    "    # Log encoding y\n",
    "    ys = torch.log10(ys)\n",
    "\n",
    "    return Batch(\n",
    "        x=xs.transpose(0, 1),\n",
    "        y=ys.transpose(0, 1),\n",
    "        target_y=ys.transpose(0, 1),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1\n",
    "seq_len = 1000\n",
    "num_features = 1\n",
    "log_start = -6\n",
    "log_end = 15\n",
    "\n",
    "# Generate data\n",
    "_, ys = sample_from_multi_modal_distribution(batch_size, seq_len, num_features)\n",
    "x = np.logspace(log_start, log_end, seq_len, dtype=np.float32)\n",
    "\n",
    "# Compute the multi-modal PDF and CDF\n",
    "rng = np.random.default_rng(seed=None)\n",
    "num_peaks = 4  # Adjust number of peaks as desired\n",
    "pdf = multi_modal_distribution(x, num_peaks, log_start, log_end, rng=rng)\n",
    "pdf /= np.trapz(pdf, x)  # Normalize PDF\n",
    "cdf = np.cumsum(pdf)\n",
    "cdf /= cdf[-1]  # Normalize CDF\n",
    "\n",
    "# Plot PDF, CDF, and sampled distribution\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# PDF plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, pdf, label=\"PDF\", color=\"blue\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Multi-Modal Uniform PDF\")\n",
    "plt.xlabel(\"x (log scale)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "# CDF plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, cdf, label=\"CDF\", color=\"green\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Cumulative Distribution Function (CDF)\")\n",
    "plt.xlabel(\"x (log scale)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# Histogram of samples\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(ys[0].numpy(), bins=50, density=True, alpha=0.7, color=\"orange\", label=\"Samples\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Sampled Distribution\")\n",
    "plt.xlabel(\"x (log scale)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../../models/final_kinpfn_model_1400_1000_1000_86_50_2.5588748050825984e-05_256_4_512_8_0.0_0.0.pt\"\n",
    "\n",
    "kinpfn = KINPFN(\n",
    "    model_path=model_path,\n",
    ")\n",
    "trained_model = kinpfn.model\n",
    "\n",
    "if trained_model is not None:\n",
    "    print(\"Load trained model!\")\n",
    "else:\n",
    "    print(\"No trained model found!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_new_distr_prior(trained_model, n_components_list):\n",
    "    set_seed(seed=123)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [10, 25, 50, 75, 100]\n",
    "\n",
    "    batch = get_batch_new_distribution_prior(100, seq_len)\n",
    "    dataset_size = 100\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    #n_components_list = [2, 3, 4, 5]\n",
    "\n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "        mae_losses = []\n",
    "        mean_nll_losses = []\n",
    "        ensemble_mae_losses = []\n",
    "        ensemble_nll_losses = []\n",
    "\n",
    "        for i in indices:\n",
    "            batch_index = i\n",
    "\n",
    "            train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "            train_x = x[train_indices, batch_index]\n",
    "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "            test_x = x[:, batch_index]\n",
    "            test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "            train_x = train_x.to(device)\n",
    "            train_y_folding_times = train_y_folding_times.to(device)\n",
    "            test_x = test_x.to(device)\n",
    "            test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Add a batch dimension as required by the transformer model\n",
    "                logits = trained_model(\n",
    "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                )\n",
    "\n",
    "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "            ground_truth_cdf = torch.arange(\n",
    "                1, len(ground_truth_sorted_folding_times) + 1\n",
    "            ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "            pred_cdf_original = (\n",
    "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "            )[0][0]\n",
    "\n",
    "            single_absolute_error = np.abs(pred_cdf_original - ground_truth_cdf)\n",
    "            mae = single_absolute_error.mean()\n",
    "            mae_losses.append(mae)\n",
    "\n",
    "            nll_loss = trained_model.criterion.forward(\n",
    "                logits=logits, y=test_y_folding_times_sorted\n",
    "            )\n",
    "            mean_nll_loss = nll_loss.mean()\n",
    "            mean_nll_losses.append(mean_nll_loss)\n",
    "\n",
    "            train_y_folding_times_np = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "            test_y_folding_times_np = test_y_folding_times_sorted.reshape(-1, 1).cpu().numpy()\n",
    "            combined_density = np.zeros_like(test_y_folding_times_np.flatten())\n",
    "            marginal_weights = []\n",
    "            gmm_pdfs = []\n",
    "\n",
    "            for n_components in n_components_list:\n",
    "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "                gmm.fit(train_y_folding_times_np)\n",
    "                \n",
    "                log_marginal_likelihood = gmm.score(train_y_folding_times_np) * len(train_y_folding_times_np)\n",
    "                marginal_weights.append(log_marginal_likelihood)\n",
    "                \n",
    "                gmm_pdf = np.exp(gmm.score_samples(test_y_folding_times_np))\n",
    "                gmm_pdfs.append(gmm_pdf)\n",
    "\n",
    "            marginal_weights = np.exp(marginal_weights - np.max(marginal_weights))\n",
    "            marginal_weights /= np.sum(marginal_weights)\n",
    "\n",
    "            # Weighted combined density\n",
    "            for gmm_pdf, weight in zip(gmm_pdfs, marginal_weights):\n",
    "                combined_density += weight * gmm_pdf\n",
    "\n",
    "            combined_cdf = np.cumsum(combined_density) / np.sum(combined_density)\n",
    "\n",
    "            ensemble_mae = np.abs(combined_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "            ensemble_mae_losses.append(ensemble_mae)\n",
    "\n",
    "            ensemble_nll = -np.mean(np.log(combined_density + 1e-9))\n",
    "            ensemble_nll_losses.append(ensemble_nll)\n",
    "\n",
    "        mae_losses = torch.tensor(mae_losses).mean()\n",
    "        print(f\"KinPFN Mean MAE Loss: {mae_losses}\")\n",
    "\n",
    "        mean_nll_losses = torch.tensor(mean_nll_losses).mean()\n",
    "        print(f\"KinPFN Mean NLL Loss: {mean_nll_losses}\")\n",
    "\n",
    "        ensemble_mae_losses = torch.tensor(ensemble_mae_losses).mean()\n",
    "        print(f\"GMM Ensemble Mean MAE Loss: {ensemble_mae_losses}\")\n",
    "\n",
    "        ensemble_nll_losses = torch.tensor(ensemble_nll_losses).mean()\n",
    "        print(f\"GMM Ensemble Mean NLL Loss: {ensemble_nll_losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model_on_new_distr_prior(trained_model,n_components_list=[2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model_on_new_distr_prior_plotting(trained_model, n_components_list):\n",
    "    set_seed(seed=123)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [10, 25, 50, 75, 100]\n",
    "\n",
    "    batch = get_batch_new_distribution_prior(100, seq_len)\n",
    "    dataset_size = 100\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "        mae_losses = []\n",
    "        mean_nll_losses = []\n",
    "        ensemble_mae_losses = []\n",
    "        ensemble_nll_losses = []\n",
    "\n",
    "        # Process the first example only for plotting\n",
    "        batch_index = indices[0]\n",
    "\n",
    "        train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "        train_x = x[train_indices, batch_index]\n",
    "        train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "        test_x = x[:, batch_index]\n",
    "        test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "        train_x = train_x.to(device)\n",
    "        train_y_folding_times = train_y_folding_times.to(device)\n",
    "        test_x = test_x.to(device)\n",
    "        test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Add a batch dimension as required by the transformer model\n",
    "            logits = trained_model(\n",
    "                train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "            )\n",
    "\n",
    "        ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "        ground_truth_cdf = torch.arange(\n",
    "            1, len(ground_truth_sorted_folding_times) + 1\n",
    "        ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "        test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "        pred_cdf_original = (\n",
    "            trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "        )[0][0]\n",
    "\n",
    "        # GMM ensemble\n",
    "        train_y_folding_times_np = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "        test_y_folding_times_np = test_y_folding_times_sorted.reshape(-1, 1).cpu().numpy()\n",
    "        combined_density = np.zeros_like(test_y_folding_times_np.flatten())\n",
    "        marginal_weights = []\n",
    "        gmm_pdfs = []\n",
    "\n",
    "        for n_components in n_components_list:\n",
    "            gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "            gmm.fit(train_y_folding_times_np)\n",
    "            \n",
    "            log_marginal_likelihood = gmm.score(train_y_folding_times_np) * len(train_y_folding_times_np)\n",
    "            marginal_weights.append(log_marginal_likelihood)\n",
    "            \n",
    "            gmm_pdf = np.exp(gmm.score_samples(test_y_folding_times_np))\n",
    "            gmm_pdfs.append(gmm_pdf)\n",
    "\n",
    "        marginal_weights = np.exp(marginal_weights - np.max(marginal_weights))\n",
    "        marginal_weights /= np.sum(marginal_weights)\n",
    "\n",
    "        # Weighted combined density\n",
    "        for gmm_pdf, weight in zip(gmm_pdfs, marginal_weights):\n",
    "            combined_density += weight * gmm_pdf\n",
    "\n",
    "        combined_cdf = np.cumsum(combined_density) / np.sum(combined_density)\n",
    "\n",
    "        # Plot the CDF approximation for both models\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(\n",
    "            ground_truth_sorted_folding_times.cpu().numpy(),\n",
    "            ground_truth_cdf.cpu().numpy(),\n",
    "            label=\"Ground Truth CDF\",\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            test_y_folding_times_sorted.cpu().numpy(),\n",
    "            pred_cdf_original.cpu().numpy(),\n",
    "            label=\"Model Predicted CDF\",\n",
    "            color=\"blue\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            test_y_folding_times_sorted.cpu().numpy(),\n",
    "            combined_cdf,\n",
    "            label=\"GMM Ensemble CDF\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "        plt.title(f\"CDF Approximation (Training Points: {training_point})\")\n",
    "        plt.xlabel(\"Folding Times\")\n",
    "        plt.ylabel(\"Cumulative Probability\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        # Break after the first example is processed for plotting\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_on_new_distr_prior_plotting(trained_model,n_components_list=[2, 3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinpfn_test_iclr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
