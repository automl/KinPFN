{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import random\n",
            "import torch\n",
            "import matplotlib.pyplot as plt\n",
            "from matplotlib.offsetbox import AnchoredText\n",
            "from matplotlib.lines import Line2D\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import numpy as np\n",
            "from pathlib import Path\n",
            "from collections import defaultdict\n",
            "\n",
            "from kinpfn.priors import Batch\n",
            "from kinpfn.model import KINPFN\n",
            "\n",
            "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
            "from sklearn.neighbors import KernelDensity\n",
            "from scipy.integrate import cumulative_trapezoid"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "def set_seed(seed=123):\n",
            "    torch.manual_seed(seed)\n",
            "    np.random.seed(seed)\n",
            "    random.seed(seed)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_dataset_size(val_set_dir):\n",
            "    dataset_size = 0\n",
            "    for subdir, _, files in os.walk(val_set_dir):\n",
            "        for file in files:\n",
            "            if file.endswith(\".csv\"):\n",
            "                dataset_size += 1\n",
            "    return dataset_size\n",
            "\n",
            "\n",
            "def get_batch_testing_folding_times(val_set_dir, seq_len=100, num_features=1, **kwargs):\n",
            "\n",
            "    dataset_size = get_dataset_size(val_set_dir)\n",
            "\n",
            "    x = torch.zeros(seq_len, dataset_size, num_features)\n",
            "    y = torch.zeros(seq_len, dataset_size)\n",
            "\n",
            "    batch_index = 0\n",
            "    length_order = []\n",
            "    file_names_in_order = []\n",
            "    for subdir, _, files in os.walk(val_set_dir):\n",
            "        for file in files:\n",
            "            if file.endswith(\".csv\"):\n",
            "                path = os.path.join(subdir, file)\n",
            "                data = pl.read_csv(\n",
            "                    path,\n",
            "                    has_header=False,\n",
            "                    columns=[2, 4],\n",
            "                    n_rows=seq_len,\n",
            "                )\n",
            "\n",
            "                folding_times = data[\"column_3\"].to_numpy()\n",
            "                sequence = data[\"column_5\"][0]\n",
            "                length_order.append(len(sequence))\n",
            "                file_names_in_order.append(file)\n",
            "                sorted_folding_times = np.sort(folding_times)\n",
            "\n",
            "                # Filter out points where x > 10^15 and x < 10^-6\n",
            "                valid_indices = (sorted_folding_times <= 10**15) & (\n",
            "                    sorted_folding_times >= 10**-6\n",
            "                )\n",
            "                sorted_folding_times = sorted_folding_times[valid_indices]\n",
            "\n",
            "                # Adjust the sequence length by sampling\n",
            "                current_seq_len = len(sorted_folding_times)\n",
            "                if current_seq_len <= 0:\n",
            "                    continue\n",
            "\n",
            "                if current_seq_len < seq_len:\n",
            "                    # Repeat the sorted_folding_times and cdf to match the sequence length (Oversampling)\n",
            "                    repeat_factor = seq_len // current_seq_len + 1\n",
            "                    sorted_folding_times = np.tile(sorted_folding_times, repeat_factor)[\n",
            "                        :seq_len\n",
            "                    ]\n",
            "                else:\n",
            "                    sorted_folding_times = sorted_folding_times[:seq_len]\n",
            "\n",
            "                x[:, batch_index, 0] = torch.tensor(np.zeros(seq_len))\n",
            "                y[:, batch_index] = torch.tensor(sorted_folding_times)\n",
            "                batch_index += 1\n",
            "\n",
            "    y = torch.log10(y)\n",
            "    return Batch(x=x, y=y, target_y=y), length_order, file_names_in_order"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model_path = \"../../../../models/final_kinpfn_model_1400_1000_1000_86_50_2.5588748050825984e-05_256_4_512_8_0.0_0.0.pt\"\n",
            "\n",
            "kinpfn = KINPFN(\n",
            "    model_path=model_path,\n",
            ")\n",
            "trained_model = kinpfn.model\n",
            "\n",
            "if trained_model is not None:\n",
            "    print(\"Load trained model!\")\n",
            "else:\n",
            "    print(\"No trained model found!\")\n",
            "    exit()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.mixture import GaussianMixture\n",
            "import numpy as np\n",
            "import torch\n",
            "\n",
            "def evaluate_model_on_testing_set_gmm_ensemble_weighted(trained_model, n_components_list):\n",
            "    set_seed(seed=123)\n",
            "\n",
            "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "    seq_len = 1000\n",
            "    training_points = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
            "\n",
            "    val_set_dir = \"../../../../kinpfn_testing_set\"\n",
            "\n",
            "    batch, length_order, file_names_in_order = get_batch_testing_folding_times(\n",
            "        val_set_dir=val_set_dir, seq_len=seq_len\n",
            "    )\n",
            "    dataset_size = get_dataset_size(val_set_dir)\n",
            "    print(f\"Dataset Size: {dataset_size}\")\n",
            "\n",
            "    x = batch.x\n",
            "    y_folding_times = batch.y\n",
            "    target_y_folding_times = batch.target_y\n",
            "\n",
            "    indices = list(range(dataset_size))\n",
            "    \n",
            "    #n_components_list = [2, 3, 4, 5]\n",
            "\n",
            "    for training_point in training_points:\n",
            "        print(f\"Training Point: {training_point}\")\n",
            "        mae_losses = []\n",
            "        mean_nll_losses = []\n",
            "        ensemble_mae_losses = []\n",
            "        ensemble_nll_losses = []\n",
            "\n",
            "        for i in indices:\n",
            "            batch_index = i\n",
            "\n",
            "            train_indices = torch.randperm(seq_len)[:training_point]\n",
            "\n",
            "            train_x = x[train_indices, batch_index]\n",
            "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
            "\n",
            "            test_x = x[:, batch_index]\n",
            "            test_y_folding_times = y_folding_times[:, batch_index]\n",
            "\n",
            "            train_x = train_x.to(device)\n",
            "            train_y_folding_times = train_y_folding_times.to(device)\n",
            "            test_x = test_x.to(device)\n",
            "            test_y_folding_times = test_y_folding_times.to(device)\n",
            "\n",
            "            with torch.no_grad():\n",
            "                # Add a batch dimension as required by the transformer model\n",
            "                logits = trained_model(\n",
            "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
            "                )\n",
            "\n",
            "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
            "            ground_truth_cdf = torch.arange(\n",
            "                1, len(ground_truth_sorted_folding_times) + 1\n",
            "            ) / len(ground_truth_sorted_folding_times)\n",
            "\n",
            "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
            "            pred_cdf_original = (\n",
            "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
            "            )[0][0]\n",
            "\n",
            "            single_absolute_error = np.abs(pred_cdf_original - ground_truth_cdf)\n",
            "            mae = single_absolute_error.mean()\n",
            "            mae_losses.append(mae)\n",
            "\n",
            "            nll_loss = trained_model.criterion.forward(\n",
            "                logits=logits, y=test_y_folding_times_sorted\n",
            "            )\n",
            "            mean_nll_loss = nll_loss.mean()\n",
            "            mean_nll_losses.append(mean_nll_loss)\n",
            "\n",
            "            train_y_folding_times_np = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
            "            test_y_folding_times_np = test_y_folding_times_sorted.reshape(-1, 1).cpu().numpy()\n",
            "            combined_density = np.zeros_like(test_y_folding_times_np.flatten())\n",
            "            marginal_weights = []\n",
            "            gmm_pdfs = []\n",
            "\n",
            "            for n_components in n_components_list:\n",
            "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
            "                gmm.fit(train_y_folding_times_np)\n",
            "                \n",
            "                log_marginal_likelihood = gmm.score(train_y_folding_times_np) * len(train_y_folding_times_np)\n",
            "                marginal_weights.append(log_marginal_likelihood)\n",
            "                \n",
            "                gmm_pdf = np.exp(gmm.score_samples(test_y_folding_times_np))\n",
            "                gmm_pdfs.append(gmm_pdf)\n",
            "\n",
            "            marginal_weights = np.exp(marginal_weights - np.max(marginal_weights))\n",
            "            marginal_weights /= np.sum(marginal_weights)\n",
            "\n",
            "            # Weighted combined density\n",
            "            for gmm_pdf, weight in zip(gmm_pdfs, marginal_weights):\n",
            "                combined_density += weight * gmm_pdf\n",
            "\n",
            "            combined_cdf = np.cumsum(combined_density) / np.sum(combined_density)\n",
            "\n",
            "            ensemble_mae = np.abs(combined_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
            "            ensemble_mae_losses.append(ensemble_mae)\n",
            "\n",
            "            ensemble_nll = -np.mean(np.log(combined_density + 1e-9))\n",
            "            ensemble_nll_losses.append(ensemble_nll)\n",
            "\n",
            "        mae_losses = torch.tensor(mae_losses).mean()\n",
            "        print(f\"KinPFN Mean MAE Loss: {mae_losses}\")\n",
            "\n",
            "        mean_nll_losses = torch.tensor(mean_nll_losses).mean()\n",
            "        print(f\"KinPFN Mean NLL Loss: {mean_nll_losses}\")\n",
            "\n",
            "        ensemble_mae_losses = torch.tensor(ensemble_mae_losses).mean()\n",
            "        print(f\"GMM Ensemble Mean MAE Loss: {ensemble_mae_losses}\")\n",
            "\n",
            "        ensemble_nll_losses = torch.tensor(ensemble_nll_losses).mean()\n",
            "        print(f\"GMM Ensemble Mean NLL Loss: {ensemble_nll_losses}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "#evaluate_model_on_testing_set_gmm_ensemble_weighted(trained_model, n_components_list=[2, 3, 4, 5])\n",
            "#evaluate_model_on_testing_set_gmm_ensemble_weighted(trained_model, n_components_list=[2, 3, 4])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "from scipy.interpolate import interp1d\n",
            "\n",
            "## Family of multi-modal Gaussian distributions\n",
            "def multi_modal_distribution(\n",
            "    x, num_peaks, log_start, log_end, rng=np.random.default_rng(seed=np.random.seed())\n",
            "):\n",
            "    theta = rng.uniform(log_start, log_end)\n",
            "    means = (rng.uniform(log_start + 1, log_end + 1, num_peaks) + theta).astype(\n",
            "        np.float32\n",
            "    )\n",
            "    stds = rng.uniform(0.1, (log_end - log_start) / 5, num_peaks).astype(np.float32)\n",
            "    distribution = np.zeros_like(x, dtype=np.float32)\n",
            "\n",
            "    for mean, std in zip(means, stds):\n",
            "        distribution += np.exp(-((np.log(x) - mean) ** 2) / (2 * std**2))\n",
            "\n",
            "    return distribution\n",
            "\n",
            "\n",
            "def sample_from_multi_modal_distribution(batch_size, seq_len, num_features):\n",
            "\n",
            "    xs = torch.zeros(batch_size, seq_len, num_features, dtype=torch.float32)\n",
            "    ys = torch.empty(batch_size, seq_len, dtype=torch.float32)\n",
            "\n",
            "    rng = np.random.default_rng(seed=np.random.seed())\n",
            "    log_start = -6\n",
            "    log_end = 15\n",
            "    x = np.logspace(log_start, log_end, seq_len, dtype=np.float32)\n",
            "\n",
            "    for i in range(batch_size):\n",
            "        num_peaks = rng.integers(2, 6)\n",
            "        own_pdf = multi_modal_distribution(x, num_peaks, log_start, log_end, rng=rng)\n",
            "        own_pdf /= np.trapz(own_pdf, x)\n",
            "        own_cdf = np.cumsum(own_pdf).astype(np.float32)\n",
            "        own_cdf /= own_cdf[-1]\n",
            "        inverse_cdf = interp1d(\n",
            "            own_cdf, x, bounds_error=False, fill_value=(x[0], x[-1]), kind=\"linear\"\n",
            "        )\n",
            "        uniform_samples = rng.uniform(0, 1, seq_len).astype(np.float32)\n",
            "        samples = inverse_cdf(uniform_samples).astype(np.float32)\n",
            "        ys[i] = torch.tensor(samples, dtype=torch.float32)\n",
            "\n",
            "    return xs, ys\n",
            "\n",
            "\n",
            "def get_batch_multi_modal_distribution_prior(\n",
            "    batch_size, seq_len, num_features=1, hyperparameters=None, **kwargs\n",
            "):\n",
            "    xs, ys = sample_from_multi_modal_distribution(\n",
            "        batch_size=batch_size, seq_len=seq_len, num_features=num_features\n",
            "    )\n",
            "    # Log encoding y\n",
            "    ys = torch.log10(ys)\n",
            "\n",
            "    return Batch(\n",
            "        x=xs.transpose(0, 1),\n",
            "        y=ys.transpose(0, 1),\n",
            "        target_y=ys.transpose(0, 1),\n",
            "    )\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "def evaluate_model_gmm_ensemble_weighted_on_prior( n_components_list):\n",
            "    set_seed(seed=123)\n",
            "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "    seq_len = 1000\n",
            "    #training_points = [10, 25, 50, 75, 100,250,500,750,1000]\n",
            "    training_points = [250,500,750,1000]\n",
            "\n",
            "    batch = get_batch_multi_modal_distribution_prior(10000, seq_len)\n",
            "    dataset_size = 10000\n",
            "    print(f\"Dataset Size: {dataset_size}\")\n",
            "\n",
            "    x = batch.x\n",
            "    y_folding_times = batch.y\n",
            "    target_y_folding_times = batch.target_y\n",
            "\n",
            "    indices = list(range(dataset_size))\n",
            "    \n",
            "    #n_components_list = [2, 3, 4, 5]\n",
            "\n",
            "    for training_point in training_points:\n",
            "        print(f\"Training Point: {training_point}\")\n",
            "        ensemble_mae_losses = []\n",
            "        ensemble_nll_losses = []\n",
            "\n",
            "        for i in indices:\n",
            "            batch_index = i\n",
            "\n",
            "            train_indices = torch.randperm(seq_len)[:training_point]\n",
            "\n",
            "            train_x = x[train_indices, batch_index]\n",
            "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
            "\n",
            "            test_x = x[:, batch_index]\n",
            "            test_y_folding_times = y_folding_times[:, batch_index]\n",
            "\n",
            "            train_x = train_x.to(device)\n",
            "            train_y_folding_times = train_y_folding_times.to(device)\n",
            "            test_x = test_x.to(device)\n",
            "            test_y_folding_times = test_y_folding_times.to(device)\n",
            "\n",
            "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
            "            ground_truth_cdf = torch.arange(\n",
            "                1, len(ground_truth_sorted_folding_times) + 1\n",
            "            ) / len(ground_truth_sorted_folding_times)\n",
            "\n",
            "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
            "\n",
            "            train_y_folding_times_np = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
            "            test_y_folding_times_np = test_y_folding_times_sorted.reshape(-1, 1).cpu().numpy()\n",
            "            combined_density = np.zeros_like(test_y_folding_times_np.flatten())\n",
            "            marginal_weights = []\n",
            "            gmm_pdfs = []\n",
            "\n",
            "            for n_components in n_components_list:\n",
            "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
            "                gmm.fit(train_y_folding_times_np)\n",
            "                \n",
            "                log_marginal_likelihood = gmm.score(train_y_folding_times_np) * len(train_y_folding_times_np)\n",
            "                marginal_weights.append(log_marginal_likelihood)\n",
            "                \n",
            "                gmm_pdf = np.exp(gmm.score_samples(test_y_folding_times_np))\n",
            "                gmm_pdfs.append(gmm_pdf)\n",
            "\n",
            "            marginal_weights = np.exp(marginal_weights - np.max(marginal_weights))\n",
            "            marginal_weights /= np.sum(marginal_weights)\n",
            "\n",
            "            # Weighted combined density\n",
            "            for gmm_pdf, weight in zip(gmm_pdfs, marginal_weights):\n",
            "                combined_density += weight * gmm_pdf\n",
            "\n",
            "            combined_cdf = np.cumsum(combined_density) / np.sum(combined_density)\n",
            "\n",
            "            ensemble_mae = np.abs(combined_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
            "            ensemble_mae_losses.append(ensemble_mae)\n",
            "\n",
            "            ensemble_nll = -np.mean(np.log(combined_density + 1e-9))\n",
            "            ensemble_nll_losses.append(ensemble_nll)\n",
            "\n",
            "        ensemble_mae_losses = torch.tensor(ensemble_mae_losses).mean()\n",
            "        print(f\"GMM Ensemble Mean MAE Loss: {ensemble_mae_losses}\")\n",
            "\n",
            "        ensemble_nll_losses = torch.tensor(ensemble_nll_losses).mean()\n",
            "        print(f\"GMM Ensemble Mean NLL Loss: {ensemble_nll_losses}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "evaluate_model_gmm_ensemble_weighted_on_prior(n_components_list=[2, 3, 4, 5])\n",
            "#evaluate_model_gmm_ensemble_weighted_on_prior(n_components_list=[2, 3, 4])"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "kinpfn_test_iclr_env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.20"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
