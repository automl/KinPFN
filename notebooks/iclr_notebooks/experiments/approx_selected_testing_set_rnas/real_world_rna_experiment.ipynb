{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from kinpfn.priors import Batch\n",
    "from kinpfn.model import KINPFN\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.integrate import cumulative_trapezoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(val_set_dir):\n",
    "    dataset_size = 0\n",
    "    for subdir, _, files in os.walk(val_set_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                dataset_size += 1\n",
    "    return dataset_size\n",
    "\n",
    "\n",
    "def get_batch_testing_folding_times(val_set_dir, seq_len=100, num_features=1, **kwargs):\n",
    "\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "\n",
    "    x = torch.zeros(seq_len, dataset_size, num_features)\n",
    "    y = torch.zeros(seq_len, dataset_size)\n",
    "\n",
    "    batch_index = 0\n",
    "    length_order = []\n",
    "    file_names_in_order = []\n",
    "    for subdir, _, files in os.walk(val_set_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                path = os.path.join(subdir, file)\n",
    "                data = pl.read_csv(\n",
    "                    path,\n",
    "                    has_header=False,\n",
    "                    columns=[2, 4],\n",
    "                    n_rows=seq_len,\n",
    "                )\n",
    "\n",
    "                folding_times = data[\"column_3\"].to_numpy()\n",
    "                sequence = data[\"column_5\"][0]\n",
    "                length_order.append(len(sequence))\n",
    "                file_names_in_order.append(file)\n",
    "                sorted_folding_times = np.sort(folding_times)\n",
    "\n",
    "                # Filter out points where x > 10^15 and x < 10^-6\n",
    "                valid_indices = (sorted_folding_times <= 10**15) & (\n",
    "                    sorted_folding_times >= 10**-6\n",
    "                )\n",
    "                sorted_folding_times = sorted_folding_times[valid_indices]\n",
    "\n",
    "                # Adjust the sequence length by sampling\n",
    "                current_seq_len = len(sorted_folding_times)\n",
    "                if current_seq_len <= 0:\n",
    "                    continue\n",
    "\n",
    "                if current_seq_len < seq_len:\n",
    "                    # Repeat the sorted_folding_times and cdf to match the sequence length (Oversampling)\n",
    "                    repeat_factor = seq_len // current_seq_len + 1\n",
    "                    sorted_folding_times = np.tile(sorted_folding_times, repeat_factor)[\n",
    "                        :seq_len\n",
    "                    ]\n",
    "                else:\n",
    "                    sorted_folding_times = sorted_folding_times[:seq_len]\n",
    "\n",
    "                x[:, batch_index, 0] = torch.tensor(np.zeros(seq_len))\n",
    "                y[:, batch_index] = torch.tensor(sorted_folding_times)\n",
    "                batch_index += 1\n",
    "\n",
    "    y = torch.log10(y)\n",
    "    return Batch(x=x, y=y, target_y=y), length_order, file_names_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../../models/final_kinpfn_model_1400_1000_1000_86_50_2.5588748050825984e-05_256_4_512_8_0.0_0.0.pt\"\n",
    "\n",
    "kinpfn = KINPFN(\n",
    "    model_path=model_path,\n",
    ")\n",
    "trained_model = kinpfn.model\n",
    "\n",
    "if trained_model is not None:\n",
    "    print(\"Load trained model!\")\n",
    "else:\n",
    "    print(\"No trained model found!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kinpfn_on_selected_testing_set_seq(trained_model, training_points, seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 100000)\n",
    "    set_seed(seed=seed)\n",
    "    print(f\"Seed: {seed}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "\n",
    "    num_evaluations = 2\n",
    "\n",
    "    val_set_dir = \"./data\"\n",
    "\n",
    "    batch, length_order, file_names_in_order = get_batch_testing_folding_times(\n",
    "        val_set_dir=val_set_dir, seq_len=seq_len\n",
    "    )\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    evaluations = 0\n",
    "    width = 7.25 * len(training_points)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_evaluations,\n",
    "        ncols=len(training_points),\n",
    "        figsize=(width, 12),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "    fig.set_dpi(300)\n",
    "    for i in range(num_evaluations):\n",
    "        row = i\n",
    "    \n",
    "        batch_index = i\n",
    "\n",
    "        for j, training_point in enumerate(training_points):\n",
    "            col = j\n",
    "            ax = axes[row, col]\n",
    "\n",
    "            all_pred_cdf_list = []\n",
    "            all_mae_list = []\n",
    "            for _ in range(20):\n",
    "                train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "                # Create the training and test data\n",
    "                train_x = x[train_indices, batch_index]\n",
    "                train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "                test_x = x[:, batch_index]\n",
    "                test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "                train_x = train_x.to(device)\n",
    "                train_y_folding_times = train_y_folding_times.to(device)\n",
    "                test_x = test_x.to(device)\n",
    "                test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = trained_model(\n",
    "                        train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                    )\n",
    "\n",
    "                ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "                ground_truth_cdf = torch.arange(\n",
    "                    1, len(ground_truth_sorted_folding_times) + 1\n",
    "                ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "                test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "                train_y_folding_times_sorted, _ = torch.sort(train_y_folding_times)\n",
    "\n",
    "                pred_cdf_original = (\n",
    "                    trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "                )[0][0]\n",
    "\n",
    "                linspace_extended = torch.linspace(\n",
    "                    train_y_folding_times_sorted[0] - 3,\n",
    "                    train_y_folding_times_sorted[-1] + 1,\n",
    "                    1000,\n",
    "                )\n",
    "                pred_cdf_linspace_extended = (\n",
    "                    trained_model.criterion.cdf(logits, linspace_extended)\n",
    "                )[0][0]\n",
    "\n",
    "                all_pred_cdf_list.append(pred_cdf_linspace_extended.cpu().numpy())\n",
    "                single_absolute_error = np.abs(pred_cdf_original.cpu().numpy() - ground_truth_cdf.cpu().numpy())\n",
    "                mae = single_absolute_error.mean()\n",
    "                all_mae_list.append(mae)\n",
    "\n",
    "            mean_pred_cdf = np.mean(all_pred_cdf_list, axis=0)\n",
    "            std_pred_cdf = np.std(all_pred_cdf_list, axis=0)\n",
    "            mean_mae = np.mean(all_mae_list)\n",
    "\n",
    "            ax.scatter(\n",
    "                10**ground_truth_sorted_folding_times.cpu().numpy(),\n",
    "                ground_truth_cdf.cpu().numpy(),\n",
    "                color=\"#000000\",\n",
    "                marker=\"x\",\n",
    "                label=\"Target\",\n",
    "            )\n",
    "\n",
    "\n",
    "            ax.plot(\n",
    "                10**linspace_extended.cpu().numpy(),\n",
    "                mean_pred_cdf,\n",
    "                color=\"#cc101fc7\",\n",
    "                marker=\".\",\n",
    "                label=\"KinPFN \" + r\"(mean $\\pm$ std)\",\n",
    "            )\n",
    "\n",
    "            ax.fill_between(\n",
    "                10**linspace_extended.cpu().numpy(),\n",
    "                mean_pred_cdf - std_pred_cdf,\n",
    "                mean_pred_cdf + std_pred_cdf,\n",
    "                color=\"#cc101fc7\",\n",
    "                alpha=0.5,\n",
    "            )\n",
    "\n",
    "            ax.scatter(\n",
    "                10**train_y_folding_times_sorted.cpu().numpy(),\n",
    "                np.zeros_like(train_y_folding_times_sorted.cpu().numpy()),\n",
    "                color=\"blue\",\n",
    "                marker=\"o\",\n",
    "                label=\"Context First Passage Times\",\n",
    "            )\n",
    "\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_xlabel(\"Time\", fontsize=18)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(\n",
    "                    \"Cumulative Population Probability\", fontsize=18\n",
    "                )\n",
    "\n",
    "            # MAE\n",
    "            single_absolute_error = np.abs(pred_cdf_original - ground_truth_cdf)\n",
    "            mae = single_absolute_error.mean()\n",
    "\n",
    "            # NLL\n",
    "            nll_loss = trained_model.criterion.forward(\n",
    "                logits=logits, y=test_y_folding_times_sorted\n",
    "            )\n",
    "            mean_nll_loss = nll_loss.mean()\n",
    "\n",
    "            print(f\"#KinPFN Context Times: {len(train_y_folding_times)}\")\n",
    "            print(f\"MAE: {mean_mae:.4f}\")\n",
    "\n",
    "            ax.legend(fontsize=15)\n",
    "\n",
    "        evaluations += 1\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_testing_set_gmm_bgmm_kde(trained_model):\n",
    "    set_seed(seed=123)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
    "\n",
    "    val_set_dir = \"../../../../kinpfn_testing_set\"\n",
    "\n",
    "    batch, length_order, file_names_in_order = get_batch_testing_folding_times(\n",
    "        val_set_dir=val_set_dir, seq_len=seq_len\n",
    "    )\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    # Iterate over different values of n_components for GMM and DP-GMM\n",
    "    n_components_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "        mae_losses = []\n",
    "        mean_nll_losses = []\n",
    "        kde_nll_losses = []\n",
    "        gmm_results = {n: [] for n in n_components_list}\n",
    "        bgmm_results = {n: [] for n in n_components_list}\n",
    "\n",
    "        for i in indices:\n",
    "            batch_index = i\n",
    "\n",
    "            train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "            # Create the training and test data\n",
    "            train_x = x[train_indices, batch_index]\n",
    "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "            test_x = x[:, batch_index]\n",
    "            test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "            train_x = train_x.to(device)\n",
    "            train_y_folding_times = train_y_folding_times.to(device)\n",
    "            test_x = test_x.to(device)\n",
    "            test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # we add our batch dimension, as our transformer always expects that\n",
    "                logits = trained_model(\n",
    "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                )\n",
    "\n",
    "            ### CALCULATE GROUND TRUTH CDF WHICH WE WANT TO PREDICT\n",
    "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "            ground_truth_cdf = torch.arange(\n",
    "                1, len(ground_truth_sorted_folding_times) + 1\n",
    "            ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "\n",
    "            ### CDF FUNCTION\n",
    "            pred_cdf_original = (\n",
    "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "            )[0][0]\n",
    "\n",
    "            # MAE\n",
    "            single_absolute_error = np.abs(pred_cdf_original - ground_truth_cdf)\n",
    "            mae = single_absolute_error.mean()\n",
    "            mae_losses.append(mae)\n",
    "\n",
    "            # NLL\n",
    "            nll_loss = trained_model.criterion.forward(\n",
    "                logits=logits, y=test_y_folding_times_sorted\n",
    "            )\n",
    "            mean_nll_loss = nll_loss.mean()\n",
    "            mean_nll_losses.append(mean_nll_loss)\n",
    "\n",
    "            # KDE\n",
    "            kde_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "            kde_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.3520031472796679).fit(kde_train_y_folding_times)\n",
    "            log_likelihood = kde.score_samples(kde_test_y_folding_times_sorted)  # Total log-likelihood\n",
    "            kde_nll = -np.mean(log_likelihood)\n",
    "            kde_nll_losses.append(kde_nll)\n",
    "\n",
    "\n",
    "            # Competitor GMM and DP-GMM for multiple n_components\n",
    "            for n_components in n_components_list:\n",
    "                # GMM\n",
    "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "                gmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "                gmm.fit(gmm_train_y_folding_times)\n",
    "                gmm_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "                gmm_nll = -gmm.score(gmm_test_y_folding_times_sorted)\n",
    "                gmm_results[n_components].append(gmm_nll)\n",
    "\n",
    "                # DP-GMM\n",
    "                bgmm = BayesianGaussianMixture(n_components=n_components, weight_concentration_prior_type=\"dirichlet_process\",weight_concentration_prior=0.0009794696670695395, max_iter=100000)\n",
    "                bgmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "                bgmm.fit(bgmm_train_y_folding_times)\n",
    "                bgmm_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "                bgmm_nll = -bgmm.score(bgmm_test_y_folding_times_sorted)\n",
    "                bgmm_results[n_components].append(bgmm_nll)\n",
    "\n",
    "        # Calculate mean MAE and NLL losses for the model\n",
    "        mae_losses = torch.stack(mae_losses)\n",
    "        mae_losses = mae_losses.mean()\n",
    "        print(f\"KinPFN Mean MAE: {mae_losses}\")\n",
    "        mean_nll_losses = torch.stack(mean_nll_losses)\n",
    "        mean_nll_losses = mean_nll_losses.mean()\n",
    "        print(f\"KinPFN Mean NLL Loss: {mean_nll_losses}\")\n",
    "\n",
    "        # Calculate mean KDE NLL loss for the model\n",
    "        kde_nll_losses = torch.tensor(kde_nll_losses)\n",
    "        kde_nll_losses = kde_nll_losses.mean()\n",
    "        print(f\"KDE Mean NLL Loss: {kde_nll_losses}\")\n",
    "\n",
    "        # Calculate and print mean NLL losses for GMM and DP-GMM for each n_component\n",
    "        for n_components in n_components_list:\n",
    "            gmm_mean_nll_losses = torch.tensor(gmm_results[n_components])\n",
    "            gmm_mean_nll_losses = gmm_mean_nll_losses.mean()\n",
    "            print(f\"GMM Mean NLL Loss (n_components={n_components}): {gmm_mean_nll_losses}\")\n",
    "\n",
    "            bgmm_mean_nll_losses = torch.tensor(bgmm_results[n_components])\n",
    "            bgmm_mean_nll_losses = bgmm_mean_nll_losses.mean()\n",
    "            print(f\"DP-GMM Mean NLL Loss (n_components={n_components}): {bgmm_mean_nll_losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KinPFN, KDE, GMM and DP-GMM on the test set\n",
    "#evaluate_model_on_testing_set_gmm_bgmm_kde(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_testing_set_gmm_bgmm_nll_mae(trained_model):\n",
    "\n",
    "    set_seed(seed=123)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
    "\n",
    "    val_set_dir = \"../../../../kinpfn_testing_set\"\n",
    "\n",
    "    batch, length_order, file_names_in_order = get_batch_testing_folding_times(\n",
    "        val_set_dir=val_set_dir, seq_len=seq_len\n",
    "    )\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    n_components_list = [2, 3, 4, 5]\n",
    "\n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "        mae_losses = []\n",
    "        mean_nll_losses = []\n",
    "        kde_mae_losses = []\n",
    "        kde_nll_losses = []\n",
    "        gmm_mae_losses = {n: [] for n in n_components_list}\n",
    "        bgmm_mae_losses = {n: [] for n in n_components_list}\n",
    "        gmm_nll_losses = {n: [] for n in n_components_list}\n",
    "        bgmm_nll_losses = {n: [] for n in n_components_list}\n",
    "\n",
    "        for i in indices:\n",
    "            batch_index = i\n",
    "\n",
    "            train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "            train_x = x[train_indices, batch_index]\n",
    "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "            test_x = x[:, batch_index]\n",
    "            test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "            train_x = train_x.to(device)\n",
    "            train_y_folding_times = train_y_folding_times.to(device)\n",
    "            test_x = test_x.to(device)\n",
    "            test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # we add our batch dimension, as our transformer always expects that\n",
    "                logits = trained_model(\n",
    "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                )\n",
    "\n",
    "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "            ground_truth_cdf = torch.arange(\n",
    "                1, len(ground_truth_sorted_folding_times) + 1\n",
    "            ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "            #test_y_folding_times_sorted = test_y_folding_times.sort().values.cpu().numpy()\n",
    "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "            pred_cdf_original = (\n",
    "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "            )[0][0]\n",
    "\n",
    "            # MAE for KinPFN\n",
    "            single_absolute_error = np.abs(pred_cdf_original - ground_truth_cdf)\n",
    "            mae = single_absolute_error.mean()\n",
    "            mae_losses.append(mae)\n",
    "\n",
    "            # NLL for KinPFN\n",
    "            nll_loss = trained_model.criterion.forward(\n",
    "                logits=logits, y=test_y_folding_times_sorted\n",
    "            )\n",
    "            mean_nll_loss = nll_loss.mean()\n",
    "            mean_nll_losses.append(mean_nll_loss)\n",
    "\n",
    "            # KDE MAE and NLL\n",
    "            kde_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.3520031472796679).fit(kde_train_y_folding_times)\n",
    "            kde_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "            kde_pdf = np.exp(kde.score_samples(kde_test_y_folding_times_sorted))\n",
    "            kde_cdf = np.cumsum(kde_pdf) / np.sum(kde_pdf)\n",
    "            kde_mae = np.abs(kde_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "            kde_mae_losses.append(kde_mae)\n",
    "\n",
    "            kde_nll = -np.mean(kde.score_samples(kde_test_y_folding_times_sorted))\n",
    "            kde_nll_losses.append(kde_nll)\n",
    "\n",
    "            # GMM and DP-GMM MAE and NLL\n",
    "            for n_components in n_components_list:\n",
    "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "                gmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "                gmm.fit(gmm_train_y_folding_times)\n",
    "                gmm_pdf = np.exp(gmm.score_samples(kde_test_y_folding_times_sorted))\n",
    "                gmm_cdf = np.cumsum(gmm_pdf) / np.sum(gmm_pdf)\n",
    "                gmm_mae = np.abs(gmm_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "                gmm_mae_losses[n_components].append(gmm_mae)\n",
    "\n",
    "                gmm_nll = -gmm.score(kde_test_y_folding_times_sorted)\n",
    "                gmm_nll_losses[n_components].append(gmm_nll)\n",
    "\n",
    "                bgmm = BayesianGaussianMixture(\n",
    "                    n_components=n_components,\n",
    "                    weight_concentration_prior_type=\"dirichlet_process\",\n",
    "                    weight_concentration_prior=0.0009794696670695395,\n",
    "                    max_iter=100000\n",
    "                )\n",
    "                bgmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "                bgmm.fit(bgmm_train_y_folding_times)\n",
    "                bgmm_pdf = np.exp(bgmm.score_samples(kde_test_y_folding_times_sorted))\n",
    "                bgmm_cdf = np.cumsum(bgmm_pdf) / np.sum(bgmm_pdf)\n",
    "                bgmm_mae = np.abs(bgmm_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "                bgmm_mae_losses[n_components].append(bgmm_mae)\n",
    "\n",
    "                bgmm_nll = -bgmm.score(kde_test_y_folding_times_sorted)\n",
    "                bgmm_nll_losses[n_components].append(bgmm_nll)\n",
    "\n",
    "        # Calculate and print losses\n",
    "        mae_losses = torch.tensor(mae_losses).mean()\n",
    "        print(f\"KinPFN Mean MAE Loss: {mae_losses}\")\n",
    "\n",
    "        mean_nll_losses = torch.tensor(mean_nll_losses).mean()\n",
    "        print(f\"KinPFN Mean NLL Loss: {mean_nll_losses}\")\n",
    "\n",
    "        kde_mae_losses = torch.tensor(kde_mae_losses).mean()\n",
    "        print(f\"KDE Mean MAE: {kde_mae_losses}\")\n",
    "\n",
    "        kde_nll_losses = torch.tensor(kde_nll_losses).mean()\n",
    "        print(f\"KDE Mean NLL Loss: {kde_nll_losses}\")\n",
    "\n",
    "        for n_components in n_components_list:\n",
    "            gmm_mae_losses_tensor = torch.tensor(gmm_mae_losses[n_components]).mean()\n",
    "            print(f\"GMM Mean MAE (n_components={n_components}): {gmm_mae_losses_tensor}\")\n",
    "\n",
    "            bgmm_mae_losses_tensor = torch.tensor(bgmm_mae_losses[n_components]).mean()\n",
    "            print(f\"DP-GMM Mean MAE (n_components={n_components}): {bgmm_mae_losses_tensor}\")\n",
    "\n",
    "            gmm_nll_losses_tensor = torch.tensor(gmm_nll_losses[n_components]).mean()\n",
    "            print(f\"GMM Mean NLL Loss (n_components={n_components}): {gmm_nll_losses_tensor}\")\n",
    "\n",
    "            bgmm_nll_losses_tensor = torch.tensor(bgmm_nll_losses[n_components]).mean()\n",
    "            print(f\"DP-GMM Mean NLL Loss (n_components={n_components}): {bgmm_nll_losses_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model_on_testing_set_gmm_bgmm_nll_mae(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_testing_set_gmm_bgmm_rebuttal_ks(trained_model):\n",
    "    set_seed(seed=123)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
    "\n",
    "    val_set_dir = \"../../../../kinpfn_testing_set\"\n",
    "\n",
    "    batch, length_order, file_names_in_order = get_batch_testing_folding_times(\n",
    "        val_set_dir=val_set_dir, seq_len=seq_len\n",
    "    )\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    n_components_list = [2, 3, 4, 5]\n",
    "\n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "        kinpfn_ks_losses = []\n",
    "        kde_ks_losses = []\n",
    "        gmm_ks_losses = {n: [] for n in n_components_list}\n",
    "        bgmm_ks_losses = {n: [] for n in n_components_list}\n",
    "\n",
    "        for i in indices:\n",
    "            batch_index = i\n",
    "\n",
    "            train_indices = torch.randperm(seq_len)[:training_point]\n",
    "\n",
    "            train_x = x[train_indices, batch_index]\n",
    "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "\n",
    "            test_x = x[:, batch_index]\n",
    "            test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "            train_x = train_x.to(device)\n",
    "            train_y_folding_times = train_y_folding_times.to(device)\n",
    "            test_x = test_x.to(device)\n",
    "            test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = trained_model(\n",
    "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                )\n",
    "\n",
    "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "            ground_truth_cdf = torch.arange(\n",
    "                1, len(ground_truth_sorted_folding_times) + 1\n",
    "            ) / len(ground_truth_sorted_folding_times)\n",
    "\n",
    "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "\n",
    "            pred_cdf_original = (\n",
    "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "            )[0][0]\n",
    "\n",
    "            # KS for KinPFN\n",
    "            ks_loss = np.max(np.abs(pred_cdf_original.cpu().numpy() - ground_truth_cdf.cpu().numpy()))\n",
    "            kinpfn_ks_losses.append(ks_loss)\n",
    "\n",
    "\n",
    "            # KDE\n",
    "            kde_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "            kde_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "            kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.3520031472796679).fit(\n",
    "                kde_train_y_folding_times\n",
    "            )\n",
    "            kde_pdf = np.exp(kde.score_samples(kde_test_y_folding_times_sorted))\n",
    "            kde_cdf = np.cumsum(kde_pdf) / np.sum(kde_pdf)  # Normalize to form CDF\n",
    "            kde_ks = np.max(np.abs(kde_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "            kde_ks_losses.append(kde_ks)\n",
    "\n",
    "            # GMM and DP-GMM\n",
    "            for n_components in n_components_list:\n",
    "                # GMM\n",
    "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "                gmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "                gmm.fit(gmm_train_y_folding_times)\n",
    "                gmm_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "                gmm_pdf = np.exp(gmm.score_samples(gmm_test_y_folding_times_sorted))\n",
    "                gmm_cdf = np.cumsum(gmm_pdf) / np.sum(gmm_pdf)  # Normalize to form CDF\n",
    "                gmm_ks = np.max(np.abs(gmm_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "                gmm_ks_losses[n_components].append(gmm_ks)\n",
    "\n",
    "                # DP-GMM\n",
    "                bgmm = BayesianGaussianMixture(\n",
    "                    n_components=n_components,\n",
    "                    weight_concentration_prior_type=\"dirichlet_process\",\n",
    "                    weight_concentration_prior=0.0009794696670695395,\n",
    "                    max_iter=100000,\n",
    "                )\n",
    "                bgmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1)\n",
    "                bgmm.fit(bgmm_train_y_folding_times)\n",
    "                bgmm_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(\n",
    "                    -1, 1\n",
    "                )\n",
    "                bgmm_pdf = np.exp(bgmm.score_samples(bgmm_test_y_folding_times_sorted))\n",
    "                bgmm_cdf = np.cumsum(bgmm_pdf) / np.sum(bgmm_pdf)  # Normalize to form CDF\n",
    "                bgmm_ks = np.max(np.abs(bgmm_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "                bgmm_ks_losses[n_components].append(bgmm_ks)\n",
    "\n",
    "        print(f\"KinPFN Mean KS Loss: {torch.tensor(kinpfn_ks_losses).mean()}\")\n",
    "        print(f\"KDE Mean KS Loss: {torch.tensor(kde_ks_losses).mean()}\")\n",
    "        for n_components in n_components_list:\n",
    "            gmm_mean_ks = torch.tensor(gmm_ks_losses[n_components]).mean()\n",
    "            print(f\"GMM Mean KS Loss (n_components={n_components}): {gmm_mean_ks}\")\n",
    "            bgmm_mean_ks = torch.tensor(bgmm_ks_losses[n_components]).mean()\n",
    "            print(f\"DP-GMM Mean KS Loss (n_components={n_components}): {bgmm_mean_ks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model_on_testing_set_gmm_bgmm_rebuttal_ks(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_testing_folding_times_with_seq_struc(val_set_dir, seq_len=100, num_features=1, **kwargs):\n",
    "\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "\n",
    "    x = torch.zeros(seq_len, dataset_size, num_features)\n",
    "    y = torch.zeros(seq_len, dataset_size)\n",
    "\n",
    "    batch_index = 0\n",
    "    length_order = []\n",
    "    file_names_in_order = []\n",
    "    sequence_order = []\n",
    "    structure_order = []\n",
    "    mfe_order = []\n",
    "    for subdir, _, files in os.walk(val_set_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                path = os.path.join(subdir, file)\n",
    "                data = pl.read_csv(\n",
    "                    path,\n",
    "                    has_header=False,\n",
    "                    columns=[0,1, 2, 4],\n",
    "                    n_rows=seq_len,\n",
    "                )\n",
    "\n",
    "                folding_times = data[\"column_3\"].to_numpy()\n",
    "                sequence = data[\"column_5\"][0]\n",
    "                length_order.append(len(sequence))\n",
    "                sequence_order.append(sequence)\n",
    "                structure_order.append(data[\"column_1\"][0])\n",
    "                mfe_order.append(data[\"column_2\"][0])\n",
    "                file_names_in_order.append(file)\n",
    "                sorted_folding_times = np.sort(folding_times)\n",
    "\n",
    "                # Filter out points where x > 10^15 and x < 10^-6\n",
    "                valid_indices = (sorted_folding_times <= 10**15) & (\n",
    "                    sorted_folding_times >= 10**-6\n",
    "                )\n",
    "                sorted_folding_times = sorted_folding_times[valid_indices]\n",
    "\n",
    "                # Adjust the sequence length by sampling\n",
    "                current_seq_len = len(sorted_folding_times)\n",
    "                if current_seq_len <= 0:\n",
    "                    continue\n",
    "\n",
    "                if current_seq_len < seq_len:\n",
    "                    # Repeat the sorted_folding_times and cdf to match the sequence length (Oversampling)\n",
    "                    repeat_factor = seq_len // current_seq_len + 1\n",
    "                    sorted_folding_times = np.tile(sorted_folding_times, repeat_factor)[\n",
    "                        :seq_len\n",
    "                    ]\n",
    "                else:\n",
    "                    sorted_folding_times = sorted_folding_times[:seq_len]\n",
    "\n",
    "                x[:, batch_index, 0] = torch.tensor(np.zeros(seq_len))\n",
    "                y[:, batch_index] = torch.tensor(sorted_folding_times)\n",
    "                batch_index += 1\n",
    "\n",
    "    y = torch.log10(y)\n",
    "    return Batch(x=x, y=y, target_y=y), length_order, file_names_in_order, sequence_order, structure_order, mfe_order\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_on_testing_set_individual_metrics(trained_model):\n",
    "    set_seed(seed=123)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    seq_len = 1000\n",
    "    training_points = [25]\n",
    "\n",
    "    val_set_dir = \"../../../../kinpfn_testing_set\"\n",
    "    batch, length_order, file_names_in_order, sequence_order, structure_order, mfe_order = get_batch_testing_folding_times_with_seq_struc(\n",
    "        val_set_dir=val_set_dir, seq_len=seq_len\n",
    "    )\n",
    "    dataset_size = get_dataset_size(val_set_dir)\n",
    "    print(f\"Dataset Size: {dataset_size}\")\n",
    "\n",
    "    x = batch.x\n",
    "    y_folding_times = batch.y\n",
    "    target_y_folding_times = batch.target_y\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    n_components_list = [2, 3, 4, 5]\n",
    "\n",
    "    # Store individual results\n",
    "    all_results = []\n",
    "\n",
    "    for training_point in training_points:\n",
    "        print(f\"Training Point: {training_point}\")\n",
    "\n",
    "        for i in indices:\n",
    "            batch_index = i\n",
    "            result = {\"example_index\": batch_index, \"training_point\": training_point}\n",
    "\n",
    "            train_indices = torch.randperm(seq_len)[:training_point]\n",
    "            train_x = x[train_indices, batch_index]\n",
    "            train_y_folding_times = y_folding_times[train_indices, batch_index]\n",
    "            test_x = x[:, batch_index]\n",
    "            test_y_folding_times = y_folding_times[:, batch_index]\n",
    "\n",
    "            # Move data to device\n",
    "            train_x = train_x.to(device)\n",
    "            train_y_folding_times = train_y_folding_times.to(device)\n",
    "            test_x = test_x.to(device)\n",
    "            test_y_folding_times = test_y_folding_times.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = trained_model(\n",
    "                    train_x[:, None], train_y_folding_times[:, None], test_x[:, None]\n",
    "                )\n",
    "\n",
    "            # Ground truth CDF\n",
    "            ground_truth_sorted_folding_times, _ = torch.sort(test_y_folding_times)\n",
    "            ground_truth_cdf = torch.arange(\n",
    "                1, len(ground_truth_sorted_folding_times) + 1\n",
    "            ) / len(ground_truth_sorted_folding_times)\n",
    "            test_y_folding_times_sorted, _ = torch.sort(test_y_folding_times)\n",
    "\n",
    "            pred_cdf_original = (\n",
    "                trained_model.criterion.cdf(logits, test_y_folding_times_sorted)\n",
    "            )[0][0]\n",
    "\n",
    "            # Metrics for KinPFN\n",
    "            kinpfn_ks = np.max(np.abs(pred_cdf_original.cpu().numpy() - ground_truth_cdf.cpu().numpy()))\n",
    "            kinpfn_mae = np.abs(pred_cdf_original - ground_truth_cdf).mean()\n",
    "            kinpfn_nll = trained_model.criterion.forward(\n",
    "                logits=logits, y=test_y_folding_times_sorted\n",
    "            ).mean().item()\n",
    "\n",
    "            result[\"kinpfn\"] = {\"KS\": kinpfn_ks, \"MAE\": kinpfn_mae.item(), \"NLL\": kinpfn_nll}\n",
    "\n",
    "            # KDE Metrics\n",
    "            kde_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "            kde_test_y_folding_times_sorted = test_y_folding_times_sorted.reshape(-1, 1)\n",
    "            kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.3520031472796679).fit(kde_train_y_folding_times)\n",
    "            kde_pdf = np.exp(kde.score_samples(kde_test_y_folding_times_sorted))\n",
    "            kde_cdf = np.cumsum(kde_pdf) / np.sum(kde_pdf)\n",
    "            kde_ks = np.max(np.abs(kde_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "            kde_mae = np.abs(kde_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "            kde_nll = -np.mean(kde.score_samples(kde_test_y_folding_times_sorted))\n",
    "\n",
    "            result[\"kde\"] = {\"KS\": kde_ks, \"MAE\": kde_mae, \"NLL\": kde_nll}\n",
    "\n",
    "            # GMM and DP-GMM Metrics\n",
    "            result[\"gmm\"] = {}\n",
    "            result[\"bgmm\"] = {}\n",
    "\n",
    "            for n_components in n_components_list:\n",
    "                # GMM\n",
    "                gmm = GaussianMixture(n_components=n_components, max_iter=100000)\n",
    "                gmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "                gmm.fit(gmm_train_y_folding_times)\n",
    "                gmm_pdf = np.exp(gmm.score_samples(kde_test_y_folding_times_sorted))\n",
    "                gmm_cdf = np.cumsum(gmm_pdf) / np.sum(gmm_pdf)\n",
    "                gmm_ks = np.max(np.abs(gmm_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "                gmm_mae = np.abs(gmm_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "                gmm_nll = -gmm.score(kde_test_y_folding_times_sorted)\n",
    "\n",
    "                result[\"gmm\"][n_components] = {\"KS\": gmm_ks, \"MAE\": gmm_mae, \"NLL\": gmm_nll}\n",
    "\n",
    "                # DP-GMM\n",
    "                bgmm = BayesianGaussianMixture(\n",
    "                    n_components=n_components,\n",
    "                    weight_concentration_prior_type=\"dirichlet_process\",\n",
    "                    weight_concentration_prior=0.0009794696670695395,\n",
    "                    max_iter=100000,\n",
    "                )\n",
    "                bgmm_train_y_folding_times = train_y_folding_times.reshape(-1, 1).cpu().numpy()\n",
    "                bgmm.fit(bgmm_train_y_folding_times)\n",
    "                bgmm_pdf = np.exp(bgmm.score_samples(kde_test_y_folding_times_sorted))\n",
    "                bgmm_cdf = np.cumsum(bgmm_pdf) / np.sum(bgmm_pdf)\n",
    "                bgmm_ks = np.max(np.abs(bgmm_cdf - ground_truth_cdf.cpu().numpy()))\n",
    "                bgmm_mae = np.abs(bgmm_cdf - ground_truth_cdf.cpu().numpy()).mean()\n",
    "                bgmm_nll = -bgmm.score(kde_test_y_folding_times_sorted)\n",
    "\n",
    "                result[\"bgmm\"][n_components] = {\"KS\": bgmm_ks, \"MAE\": bgmm_mae, \"NLL\": bgmm_nll}\n",
    "\n",
    "            # Append sequence and stop structure and mfe\n",
    "            result[\"sequence\"] = sequence_order[batch_index]\n",
    "            result[\"stop_structure\"] = structure_order[batch_index]\n",
    "            result[\"mfe\"] = mfe_order[batch_index]\n",
    "\n",
    "            # Add to all results\n",
    "            all_results.append(result)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "results = evaluate_model_on_testing_set_individual_metrics(trained_model)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KinPFN on 97 and 119 nucleotide long RNA in Appendix\n",
    "plot_kinpfn_on_selected_testing_set_seq(trained_model, [10, 25, 50, 75], seed=79539)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinpfn_test_iclr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
